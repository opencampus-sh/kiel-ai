# Week 7 - Natural Language Processing, Part II

### This week you will...

* start digging into a variety of model formats that are used in training models to understand context in sequence. In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'.
* learn about using natural language processing (NLP) models for predictions. Given a body of words, you could conceivably predict the word most likely to follow a given word or phrase, and once you've done that, to do it again, and again. With that in mind, you'll build a text generator. It's trained with texts that mimic the style of master Yoda from Star Wars and can be used to produce sentences that sound similar those of Yoda.

### Learning Resources

* Week 3 and 4 of the course [Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow)

### Until next week you should...

* prepare questions for the instructor team on problems in your project or on how to potentially improve your results.
* working on the definition of your project's final model(s) and their evaluation according to the instructions given [here](https://github.com/opencampus-sh/ml-project-template/blob/main/3\_Model/INSTRUCTIONS.md)
* document how you will evaluate your model results

Also, you may already start to...

* complete week 1 and week 2 of the course [Sequences, Time Series and Prediction](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction)
* complete Exercise 1 (_Exercise 2 in the same notebook is for next week_) assignments in [this notebook](https://colab.research.google.com/github/opencampus-sh/course-material/blob/main/machine-learning-with-tensorflow/week-07-and-08/Week7-8\_Notebook\_Weather-Time-Series.ipynb).
