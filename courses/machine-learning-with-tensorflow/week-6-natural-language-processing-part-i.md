# Week 6 - Natural Language Processing, Part I

### This week you will...

* get an understanding for the importance of tokenization of a text when training a neural network for texts, for example, to do a sentiment analysis. Tokenization is the process of converting the text into numeric values, with a number representing a word or a character.
* learn about embeddings, where the text tokens are mapped as vectors in a high dimensional space. With embeddings and labelled examples, these vectors can then be tuned so that words with similar meaning will have a similar direction in the vector space. This will begin the process of training a neural network to understand sentiment in text -- and you'll begin by looking at movie reviews, training a neural network on texts that are labelled 'positive' or 'negative' and determining which words in a sentence drive those meanings.

### Learning Resources

{% file src="../../.gitbook/assets/241205_NLP in TensorFlow-Part-I.pdf" %}

* Week 1 and 2 of the course [Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow)

### Until next week you should...

* complete week 3 and week 4 of the course [Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow)
* complete [this notebook](https://colab.research.google.com/github/opencampus-sh/course-material/blob/main/machine-learning-with-tensorflow/week-06/Week6_Notebook_Yoda-Corpus.ipynb) to generate text in the unique speaking style of Star Wars character Master Yoda.
* consider a baseline model or a baseline comparison for your project task according to the instructions given [here](https://github.com/opencampus-sh/ml-project-template/blob/main/1_DatasetCharacteristics/INSTRUCTIONS.md)
* document the evaluation results of your baseline model and the used metric(s) in your project repository
